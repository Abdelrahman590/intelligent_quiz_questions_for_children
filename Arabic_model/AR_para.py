from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import json

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø¯ÙŠÙ„ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
model_name = "salti/arabic-t5-small-question-paraphrasing"

try:
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print(f"âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ø¥Ø°Ø§ Ù…ØªØ§Ø­
    
    paraphraser = pipeline(
        "text2text-generation", 
        model=model, 
        tokenizer=tokenizer
    )

    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")

    def generate_paraphrases(question, num_versions=2):
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ§Øª Ù„Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£ØµÙ„"""
        paraphrases = paraphraser(
            question,
            max_new_tokens=80,
            num_return_sequences=num_versions,
            num_beams=5,
            repetition_penalty=2.0,
            temperature=0.7  # ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        )
        return [p['generated_text'] for p in paraphrases]

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù
    input_file = "D:\\company\\arabic_questions.json"
    print(f"ğŸ“‚ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        questions = json.load(f)

    print(f"ğŸ” Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(questions)} Ø³Ø¤Ø§Ù„...")

    for i, item in enumerate(questions, 1):
        original = item.get('question_ar') or item.get('Ø§Ù„Ø³Ø¤Ø§Ù„')  # Ø¯Ø¹Ù… Ø§Ù„Ù…ÙØªØ§Ø­ÙŠÙ†
        try:
            versions = [original] + generate_paraphrases(original, num_versions=2)
            item['versions_ar'] = versions
            print(f"[{i}/{len(questions)}] âœ“ ØªÙ…Øª Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ©: {original}")
        except Exception as e:
            print(f"[{i}/{len(questions)}] âœ— Ø®Ø·Ø£ ÙÙŠ: {original} - {str(e)}")
            item['versions_ar'] = [original]

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    output_file = 'enhanced_questions1.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(questions, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡! Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ {output_file}")
    print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(questions)}")
    print(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ§Ø­Ø©: {sum(len(item['versions_ar']) for item in questions)}")

except Exception as e:
    print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¬Ø³ÙŠÙ…: {str(e)}")
    print("Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:")
    print("1. ØªØ£ÙƒØ¯ Ù…Ù† Ø§ØªØµØ§Ù„Ùƒ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª")
    print("2. Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª: pip install --upgrade transformers torch")
    print("3. Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§")
    print("4. ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„ÙØ§Øª")
